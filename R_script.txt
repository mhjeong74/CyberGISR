

A. LAB 1
module avail
module load binutils/2.25 openblas/0.2.14 R/3.2.1-openmpi geos

module list 

R

#Define a simple R function
myProc <- function(size=10000000) {
#Load a large vector
vec <- rnorm(size)
#Now sleep on it
Sys.sleep(2)
#Now sum the vec values
return(sum(vec))
}

1. Serial - apply
ptm <- proc.time()
result <- sapply(1:10, function(i) myProc())
proc.time() - ptm



2. parallel package: mclapply

require(parallel)
ptm <- proc.time()
result <- mclapply(1:10, function(i) myProc(), mc.cores=10)
proc.time() - ptm


3. snow package
require(snow)
hostnames <- rep('localhost', 10)
cluster <- makeSOCKcluster(hostnames)
clusterExport(cluster, list('myProc'))
ptm <- proc.time()
result <- clusterApply(cluster, 1:10, function(i) myProc())
proc.time() - ptm
stopCluster(cluster)

3.1 Load balancing
require(snow)
#clusterApplyLB
hostnames <- rep('localhost', 4)
cluster <- makeSOCKcluster(hostnames)
clusterExport(cluster, list('myProc'))
set.seed(777442)
sleeptime <- abs(rnorm(15,15,15))
tm <- snow.time(clusterApplyLB(cluster, sleeptime, Sys.sleep))
plot(tm)
stopCluster(cluster)

#clusterApply
hostnames <- rep('localhost', 4)
cluster <- makeSOCKcluster(hostnames)
clusterExport(cluster, list('myProc'))
set.seed(777442)
sleeptime <- abs(rnorm(15,15,15))
tm <- snow.time(clusterApply(cluster, sleeptime, Sys.sleep))
plot(tm)
stopCluster(cluster)



4. foreach + snow package
require(foreach)
require(doSNOW)
## Loading required package: doSNOW
hostnames <- rep('localhost', 10)
cluster <- makeSOCKcluster(hostnames)
registerDoSNOW(cluster)
ptm <- proc.time()
result <- foreach(i=1:10, .combine=c) %dopar% {
myProc()
}
proc.time() - ptm
stopCluster(cluster)


5.  Executing snow programs on a cluster with Rmpi 

require(Rmpi)
require(snow)

# Initialize SNOW using MPI communication. The first line will get the
# number of MPI processes the scheduler assigned to us. Everything else 
# is standard SNOW

#np <- mpi.universe.size()
#cluster <- makeMPIcluster(np)

cluster <- makeCluster(8, type="MPI")

# Print the hostname for each cluster member
sayhello <- function()
{
    info <- Sys.info()[c("nodename", "machine")]
    paste("Hello from", info[1], "with CPU type", info[2])
}

names <- clusterCall(cluster, sayhello)
print(unlist(names))

# Compute row sums in parallel using all processes,
# then a grand sum at the end on the master process
parallelSum <- function(m, n)
{
    A <- matrix(rnorm(m*n), nrow = m, ncol = n)
    row.sums <- parApply(cluster, A, 1, sum)
    print(sum(row.sums))
}

parallelSum(500, 500)

stopCluster(cluster)
mpi.exit()
#exit R
quit()

6.  foreach + snow package for multi-node
#Set up environmental setting (Before setting the environmental variable, you must quit R)
vim ~/.bashrc

#Put the information to load module R and then save the file
module load R


#Assign the number of nodes. 
 qsub -I -l walltime=48:00:00,nodes=1:ppn=20 -N kde7_2


#Start R
module load binutils/2.25 openblas/0.2.14 R/3.2.1-openmpi
R
require(foreach)
require(doSNOW)

#Define a simple R function
myProc <- function(size=1000) {
#Load a large vector
vec <- rnorm(size)
#Now sleep on it
Sys.sleep(2)
#Now sum the vec values
return(sum(vec))
}

#Get backend hostnames
nodelist <- Sys.getenv("PBS_NODEFILE")
hostnames <- scan(nodelist, what="", sep="\n")

#Set reps to match core count'
num.cores <- 10
hostnames <- rep(hostnames, each=num.cores)
hostnames
cluster <- makeSOCKcluster(hostnames)
registerDoSNOW(cluster)
ptm <- proc.time()
result <- foreach(i=1:100, .combine=c) %dopar% {
myProc()
}
proc.time() - ptm
stopCluster(cluster)



7. Bootstrap calculations
#Serial implementation
random.data <- matrix(rnorm(1000000), ncol = 1000)
bmed <- function(d, n) median(d[n])
library(boot)
sapply(1:100, function(n) {sd(boot(random.data[, n], bmed, R = 2000)$t)})


#Parallel implementation
random.data <- matrix(rnorm(1000000), ncol = 1000)
bmed <- function(d, n) median(d[n])
library(boot)
cluster = makeCluster(10, type = "SOCK")
registerDoSNOW(cluster)
clusterExport(cluster, c("random.data", "bmed"))
results = foreach(n = 1:100, .combine = c) %dopar% {
     library(boot); sd(boot(random.data[, n], bmed, R = 2000)$t)
}
stopCluster(cluster)


LAB2: Parallel spatial autocorrelation 
require(maptools) 
require(spdep)

1. Serial implementation
#an absolute filepath representing the current working directory of the R proces
getwd()
#set the working directory
setwd("~/cybergis_R")  

#Get shapefile header information in the radiation data
getinfo.shape("dataset/Oct_17_20_proj.shp")
#Reads data from a points shapefile
radiation<-readShapePoints ("dataset/Oct_17_20_proj.shp")

#Retrieve spatial coordinates from a Spatial object
coords<-coordinates(radiation)
IDs<-row.names(as(radiation, "data.frame"))

#Neighbourhood contiguity by distance
radiation_nei<-dnearneigh(coords, d1=0, d2=20, row.names=IDs)

#Spatial weights for neighbours lists
radiation_nbq_wb<-nb2listw(radiation_nei, style="W")

#Moran's I test for spatial autocorrelation
moran.test(radiation$field_5, listw=radiation_nbq_wb)

#Permutation test for Moran's I statistic
gamma <- radiation$field_5
listw <- radiation_nbq_wb  
nsim <- 999
ptm <- proc.time()
sim1 <- moran.mc(gamma, listw=listw, nsim=nsim)
proc.time() - ptm
sim1

2. Parallel Moran's I
require(foreach)
require(doSNOW)
n <- length(listw$neighbours)
S0 <- Szero(listw)
cluster = makeCluster(10, type = "SOCK")
registerDoSNOW(cluster)
clusterExport(cluster, c("gamma", "listw","n","S0"))
ptm <- proc.time()
results = foreach(n = 1:nsim, .combine = c) %dopar% {
 library(spdep); moran(sample(gamma), listw, n, S0,zero.policy=NULL)$I
}
proc.time() - ptm

paMoran <- function(res, x, listw, nsim,zero.policy=NULL,alternative="greater") {
  n <- length(listw$neighbours)
  S0 <- Szero(listw)

  res[nsim+1] <- moran(x, listw, n, S0, zero.policy)$I
  rankres <- rank(res)
  xrank <- rankres[length(res)]
	diff <- nsim - xrank
	diff <- ifelse(diff > 0, diff, 0)

  if (alternative == "less") 
        	pval <- punif((diff + 1)/(nsim + 1), lower.tail=FALSE)
    	else if (alternative == "greater") 
        	pval <- punif((diff + 1)/(nsim + 1))
  
	statistic <- res[nsim+1]
	names(statistic) <- "statistic"
	parameter <- xrank
	names(parameter) <- "observed rank"
	method <- "Parallel Monte-Carlo simulation of Moran's I"
	lres <- list(statistic=statistic, parameter=parameter,
	    p.value=pval, alternative=alternative, method=method,res=res)  
  	lres 
}
#example1
mtest <- paMoran(results,gamma,listw,nsim)
mtest$method
mtest$statistic 
mtest$parameter
mtest$p.value
#mtest$res

stopCluster(cluster)


#LAB3:
#vi .bashrc
export R_LIBS=/home/mhjeong/R

Sys.setenv(HADOOP_CMD="/usr/bin/hadoop")
Sys.setenv(HADOOP_STREAMING="/usr/hdp/2.3.2.0-2602/hadoop-mapreduce/hadoop-streaming-2.7.1.2.3.2.0-2602.jar")


